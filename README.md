# gradio_tutorial
Introduce the usage of gradio.
- [gradio\_tutorial](#gradio_tutorial)
  - [Audio:](#audio)
    - [Description(æè¿°):](#descriptionæè¿°)
    - [Behavior(è¡Œä¸º):](#behaviorè¡Œä¸º)
  - [Real Time Speech Recognition(å®æ—¶è¯­éŸ³è¯†åˆ«):](#real-time-speech-recognitionå®æ—¶è¯­éŸ³è¯†åˆ«)
    - [Introduction(ä»‹ç»):](#introductionä»‹ç»)
    - [Prerequisites(å…ˆå†³æ¡ä»¶):](#prerequisiteså…ˆå†³æ¡ä»¶)
      - [1. Set up the Transformers ASR Model(è®¾ç½®Transformers ASRæ¨¡å‹):](#1-set-up-the-transformers-asr-modelè®¾ç½®transformers-asræ¨¡å‹)
      - [2. Create a Full-Context ASR Demo with Transformers(ä½¿ç”¨Transformersåˆ›å»ºä¸€ä¸ªå®Œæ•´ä¸Šä¸‹æ–‡ASRæ¼”ç¤º):](#2-create-a-full-context-asr-demo-with-transformersä½¿ç”¨transformersåˆ›å»ºä¸€ä¸ªå®Œæ•´ä¸Šä¸‹æ–‡asræ¼”ç¤º)
      - [3. Create a Streaming ASR Demo with Transformers(ä½¿ç”¨Transformersåˆ›å»ºä¸€ä¸ªæµå¼ASRæ¼”ç¤º):](#3-create-a-streaming-asr-demo-with-transformersä½¿ç”¨transformersåˆ›å»ºä¸€ä¸ªæµå¼asræ¼”ç¤º)
      - [ä¸ªäººæ”¹ç‰ˆ--æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªç‰‡æ®µ:](#ä¸ªäººæ”¹ç‰ˆ--æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªç‰‡æ®µ)

## Audio:

```python
gradio.Audio(Â·Â·Â·)
```

### Description(æè¿°):

Creates an audio component that can be used to **upload/record** audio (**as an input**) or display audio (as an output).<br>

åˆ›å»ºä¸€ä¸ªéŸ³é¢‘ç»„ä»¶ï¼Œå¯ä»¥ç”¨äº **ä¸Šä¼ /å½•åˆ¶éŸ³é¢‘**ï¼ˆ**ä½œä¸ºè¾“å…¥**ï¼‰æˆ–æ˜¾ç¤ºéŸ³é¢‘ï¼ˆä½œä¸ºè¾“å‡ºï¼‰ã€‚<br>

### Behavior(è¡Œä¸º):

**As input component(ä½œä¸ºè¾“å…¥ç»„ä»¶):**<br>

passes audio as one of these formats (depending on type)(ä»¥ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ä¼ é€’éŸ³é¢‘ï¼ˆå–å†³äºç±»å‹ï¼‰):<br>

ğŸš€ğŸš€ğŸš€a str filepath, or tuple of (sample rate in Hz, audio data as numpy array).<br>

ğŸš€ğŸš€ğŸš€å­—ç¬¦ä¸²æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ï¼ˆé‡‡æ ·ç‡ï¼ˆä»¥èµ«å…¹ä¸ºå•ä½ï¼‰ï¼ŒéŸ³é¢‘æ•°æ®ä½œä¸ºnumpyæ•°ç»„ï¼‰çš„å…ƒç»„ã€‚<br>

If the latter, the audio data is a 16-bit int array whose values range from -32768 to 32767 and shape of the audio data array is (samples,) for mono audio or (samples, channels) for multi-channel audio.<br>

å¦‚æœæ˜¯åè€…ï¼Œåˆ™éŸ³é¢‘æ•°æ®æ ¼å¼ä¸º `dtype=int16`ï¼Œå…¶å€¼èŒƒå›´ä»-32768åˆ°32767ï¼Œå¹¶ä¸”éŸ³é¢‘æ•°æ®æ•°ç»„çš„å½¢çŠ¶ä¸ºï¼ˆæ ·æœ¬ï¼Œï¼‰ç”¨äºå•å£°é“éŸ³é¢‘ï¼Œæˆ–ï¼ˆæ ·æœ¬ï¼Œé€šé“ï¼‰ç”¨äºå¤šé€šé“éŸ³é¢‘ã€‚<br>

Your function should accept one of these types(ä½ çš„å‡½æ•°åº”è¯¥æ¥å—ä»¥ä¸‹ç±»å‹ä¹‹ä¸€):<br>

```python
def predict(
	value: str | tuple[int, np.ndarray] | None
)
	...
```

**As output component(ä½œä¸ºè¾“å‡ºç»„ä»¶):** <br>

expects audio data in any of these formats(æœŸæœ›ä»¥ä»¥ä¸‹ä»»ä¸€æ ¼å¼æä¾›éŸ³é¢‘æ•°æ®):<br>

a `str` or `pathlib.Path` filepath or `URL` to an audio file, or a bytes object (recommended for streaming), or a tuple of (sample rate in Hz, audio data as numpy array).<br>

å­—ç¬¦ä¸²æˆ–pathlib.Pathæ–‡ä»¶è·¯å¾„æˆ–éŸ³é¢‘æ–‡ä»¶çš„URLï¼Œæˆ–å­—èŠ‚å¯¹è±¡ï¼ˆæ¨èç”¨äºæµå¼ä¼ è¾“ï¼‰ï¼Œæˆ–ï¼ˆä»¥èµ«å…¹ä¸ºå•ä½çš„é‡‡æ ·ç‡ï¼ŒéŸ³é¢‘æ•°æ®ä½œä¸ºnumpyæ•°ç»„ï¼‰çš„å…ƒç»„ã€‚<br>

Note: if audio is supplied as a numpy array, the audio will be normalized by its peak value to avoid distortion or clipping in the resulting audio.<br>

æ³¨æ„ï¼šå¦‚æœéŸ³é¢‘ä»¥numpyæ•°ç»„çš„å½¢å¼æä¾›ï¼Œåˆ™éŸ³é¢‘å°†é€šè¿‡å…¶å³°å€¼è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥é¿å…ç»“æœéŸ³é¢‘ä¸­çš„å¤±çœŸæˆ–å‰ªåˆ‡ã€‚<br>

Your function should return one of these types(ä½ çš„å‡½æ•°åº”è¯¥è¿”å›ä»¥ä¸‹ç±»å‹ä¹‹ä¸€):<br>

```python
def predict(Â·Â·Â·) -> str | Path | bytes | tuple[int, np.ndarray] | None
	...	
	return value
```


## Real Time Speech Recognition(å®æ—¶è¯­éŸ³è¯†åˆ«):

### Introduction(ä»‹ç»):

Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving(éå¸¸æˆåŠŸ;è“¬å‹ƒå‘å±•) area of machine learning.<br>

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œå³å°†å£è¯­è½¬æ¢æˆæ–‡æœ¬ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸é‡è¦ä¸”è“¬å‹ƒå‘å±•çš„é¢†åŸŸã€‚<br>

ASR algorithms run on practically(å‡ ä¹;å®é™…ä¸Š) every smartphone, and are becoming increasingly embedded in professional workflows(å·¥ä½œæµç¨‹), such as digital(æ•°å­—çš„) assistants for nurses and doctors.<br>

ASRç®—æ³•å‡ ä¹è¿è¡Œåœ¨æ¯ä¸€éƒ¨æ™ºèƒ½æ‰‹æœºä¸Šï¼Œå¹¶ä¸”è¶Šæ¥è¶Šå¤šåœ°è¢«åµŒå…¥åˆ°ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­ï¼Œæ¯”å¦‚åŒ»ç”Ÿå’ŒæŠ¤å£«çš„æ•°å­—åŠ©æ‰‹ã€‚<br>

Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate(éªŒè¯) that they are behaving as expected when confronted(é¢å¯¹;é­é‡) with a wide variety of speech patterns (different accents(å£éŸ³), pitches(éŸ³è°ƒ), and background audio conditions).<br>

ç”±äºASRç®—æ³•æ˜¯è®¾è®¡ç»™å®¢æˆ·å’Œæœ€ç»ˆç”¨æˆ·ç›´æ¥ä½¿ç”¨çš„ï¼Œå› æ­¤éªŒè¯å®ƒä»¬åœ¨é¢å¯¹å„ç§å„æ ·çš„è¯­éŸ³æ¨¡å¼ï¼ˆä¸åŒçš„å£éŸ³ã€éŸ³è°ƒå’ŒèƒŒæ™¯éŸ³é¢‘æ¡ä»¶ï¼‰æ—¶æ˜¯å¦å¦‚é¢„æœŸé‚£æ ·è¿è¡Œæ˜¯éå¸¸é‡è¦çš„ã€‚<br>

Using **gradio**, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone(éº¦å…‹é£) on your device.<br>

ä½¿ç”¨ **gradio** ï¼Œä½ å¯ä»¥è½»æ¾åœ°æ„å»ºä¸€ä¸ªASRæ¨¡å‹çš„æ¼”ç¤ºï¼Œå¹¶ä¸æµ‹è¯•å›¢é˜Ÿåˆ†äº«ï¼Œæˆ–è€…é€šè¿‡è®¾å¤‡ä¸Šçš„éº¦å…‹é£äº²è‡ªæµ‹è¯•ã€‚<br>

This tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface.<br>

æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•æä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­éŸ³è½¬æ–‡æœ¬æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨Gradioç•Œé¢éƒ¨ç½²å®ƒã€‚<br>

ğŸš¨ğŸš¨ğŸš¨We will start with **a full-context model**, in which the user speaks the entire audio before the prediction runs.<br>

ğŸš¨ğŸš¨ğŸš¨æˆ‘ä»¬å°†ä»ä¸€ä¸ªå®Œæ•´ä¸Šä¸‹æ–‡æ¨¡å‹å¼€å§‹ï¼Œç”¨æˆ·éœ€è¦åœ¨ "é¢„æµ‹(å‡½æ•°)" è¿è¡Œä¹‹å‰ "è¯´å‡ºæ•´ä¸ªéŸ³é¢‘"ã€‚<br>

> æ„æ€æ˜¯ç­‰éŸ³é¢‘å…¨éƒ¨è¾“å…¥å®Œäº†ï¼Œå†è§£æï¼ŸğŸ¤¨ğŸ¤¨ğŸ¤¨

ğŸ”¥ğŸ”¥ğŸ”¥Then we will adapt the demo to make it streaming, meaning that the audio model will convert speech as you speak.<br>

ğŸ”¥ğŸ”¥ğŸ”¥ç„¶åï¼Œæˆ‘ä»¬å°†è°ƒæ•´æ¼”ç¤ºï¼Œä½¿å…¶æˆä¸ºæµå¼çš„ï¼Œæ„å‘³ç€éŸ³é¢‘æ¨¡å‹å°†åœ¨ä½ è¯´è¯æ—¶è½¬æ¢è¯­éŸ³ã€‚<br>

> è¿™éƒ¨åˆ†æ‰æ˜¯é‡ç‚¹ğŸŒˆğŸŒˆğŸŒˆ

### Prerequisites(å…ˆå†³æ¡ä»¶):

Make sure you have the gradio Python package already installed. You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:<br>

ç¡®ä¿ä½ å·²ç»å®‰è£…äº†gradio PythonåŒ…ã€‚ä½ è¿˜éœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»2ä¸ªASRåº“æ„å»ºæ¼”ç¤ºï¼š<br>

- Transformers (for this, `pip install transformers` and `pip install torch`)

Transformersï¼ˆä¸ºæ­¤ï¼Œæ‰§è¡Œ `pip install transformers` å’Œ `pip install torch` å‘½ä»¤ï¼‰

Make sure you have at least one of these installed so that you can follow along the tutorial.<br>

ç¡®ä¿ä½ è‡³å°‘å®‰è£…äº†å…¶ä¸­ä¸€ä¸ªï¼Œä»¥ä¾¿ä½ èƒ½å¤Ÿè·Ÿéšæœ¬æ•™ç¨‹ã€‚<br>

You will also need [**ffmpeg**](https://www.ffmpeg.org/download.html) installed on your system, if you do not already have it, to process files from the microphone.<br>

å¦‚æœä½ çš„ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰å®‰è£…ffmpegï¼Œä½ ä¹Ÿéœ€è¦å®‰è£…å®ƒï¼Œä»¥å¤„ç†æ¥è‡ªéº¦å…‹é£çš„æ–‡ä»¶ã€‚<br>

```txt
Notes:

`ffmpeg`æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å¼€æºå·¥å…·ï¼Œç”¨äºå¤„ç†è§†é¢‘å’ŒéŸ³é¢‘æ–‡ä»¶ã€‚å®ƒæ”¯æŒå‡ ä¹æ‰€æœ‰çš„è§†é¢‘å’ŒéŸ³é¢‘æ ¼å¼è½¬æ¢ï¼Œèƒ½å¤Ÿå½•åˆ¶ã€è½¬ç ä»¥åŠæµåŒ–æ’­æ”¾éŸ³è§†é¢‘ã€‚

`ffmpeg`å¯ä»¥ç”¨äºè§†é¢‘æ ¼å¼è½¬æ¢ã€è§†é¢‘ç¼–è§£ç ã€éŸ³è§†é¢‘åˆå¹¶ã€è§†é¢‘å‰ªè¾‘ã€å‹ç¼©ä»¥åŠå¤šç§éŸ³è§†é¢‘å¤„ç†åŠŸèƒ½ã€‚ç”±äºå…¶åŠŸèƒ½å¼ºå¤§ä¸”ä½¿ç”¨çµæ´»ï¼Œå®ƒè¢«å¹¿æ³›åº”ç”¨äºå¤šåª’ä½“å†…å®¹å¤„ç†ã€æ•°å­—åª’ä½“å¼€å‘ã€è§†é¢‘ç½‘ç«™å’Œåº”ç”¨ç¨‹åºå¼€å‘ç­‰é¢†åŸŸã€‚
```

Hereâ€™s how to build a real time speech recognition (ASR) app:<br>

ä»¥ä¸‹æ˜¯æ„å»ºå®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åº”ç”¨çš„æ–¹æ³•ï¼š<br>

#### 1. Set up the Transformers ASR Model(è®¾ç½®Transformers ASRæ¨¡å‹):

First, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model.<br>

é¦–å…ˆï¼Œä½ éœ€è¦æœ‰ä¸€ä¸ªä½ è‡ªå·±è®­ç»ƒçš„ASRæ¨¡å‹ï¼Œæˆ–è€…ä½ éœ€è¦ä¸‹è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚<br>

In this tutorial, we will start by using a pretrained ASR model from the model, **whisper**.<br>

åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»ä½¿ç”¨ä¸€ä¸ªåä¸º **whisper** çš„é¢„è®­ç»ƒASRæ¨¡å‹å¼€å§‹ã€‚<br>

Here is the code to load whisper from Hugging Face transformers:<br>

è¿™æ˜¯ä»Hugging Face transformersåŠ è½½whisperæ¨¡å‹çš„ä»£ç :<br>

```python
from transformers import pipeline

p = pipeline("automatic-speech-recognition", model="openai/whisper-base.en")
```

Thatâ€™s it!<br>

å°±è¿™æ ·ï¼<br>

#### 2. Create a Full-Context ASR Demo with Transformers(ä½¿ç”¨Transformersåˆ›å»ºä¸€ä¸ªå®Œæ•´ä¸Šä¸‹æ–‡ASRæ¼”ç¤º):

We will start by creating a full-context ASR demo, in which the user speaks the full audio before using the ASR model to run inference.<br>

æˆ‘ä»¬å°†ä»åˆ›å»ºä¸€ä¸ªå®Œæ•´ä¸Šä¸‹æ–‡ASRæ¼”ç¤ºå¼€å§‹ï¼Œåœ¨æ­¤æ¼”ç¤ºä¸­ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨ASRæ¨¡å‹è¿›è¡Œæ¨ç†ä¹‹å‰ï¼Œä¼šå…ˆè¯´å®Œæ•´ä¸ªéŸ³é¢‘ã€‚<br>

This is very easy with Gradio â€” we simply create a function around the pipeline object above.<br>

ä½¿ç”¨Gradioéå¸¸ç®€å•â€”â€”æˆ‘ä»¬åªéœ€å›´ç»•ä¸Šè¿°pipelineå¯¹è±¡åˆ›å»ºä¸€ä¸ªå‡½æ•°ã€‚<br>

We will use **gradio**â€™s built in **Audio** component(ç»„ä»¶), configured(é…ç½®) to take input from the userâ€™s microphone and return a filepath for the recorded audio.<br>

æˆ‘ä»¬å°†ä½¿ç”¨gradioå†…ç½®çš„Audioç»„ä»¶ï¼Œé…ç½®å®ƒä»ç”¨æˆ·çš„éº¦å…‹é£æ¥æ”¶è¾“å…¥ï¼Œå¹¶è¿”å›å½•åˆ¶éŸ³é¢‘çš„æ–‡ä»¶è·¯å¾„ã€‚<br>

The output component will be a plain `Textbox`.<br>

è¾“å‡ºç»„ä»¶å°†æ˜¯ä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ¡†ã€‚<br>

```python
import gradio as gr
from transformers import pipeline
import numpy as np

transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-base.en")

def transcribe(audio):
    sr, y = audio
    y = y.astype(np.float32)
    y /= np.max(np.abs(y))

    return transcriber({"sampling_rate": sr, "raw": y})["text"]


demo = gr.Interface(
    transcribe,
    gr.Audio(sources=["microphone"]),
    "text",
)

demo.launch()
```

The **transcribe** function takes a single parameter `audio` which is a `numpy` array of the audio the user recorded.<br>

> transcribe: v. è®°å½•ï¼›æŠ„å½•ï¼›æŠ„å†™ï¼›æŠŠâ€¦è½¬æˆ(å¦ä¸€ç§ä¹¦å†™å½¢å¼)ï¼›æ”¹ç¼–(ä¹æ›²ï¼Œä»¥é€‚åˆå…¶ä»–ä¹å™¨æˆ–å£°éƒ¨)ï¼›ç”¨éŸ³æ ‡æ ‡éŸ³

`transcribe` å‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•° `audio`ï¼Œè¿™æ˜¯ç”¨æˆ·å½•åˆ¶çš„éŸ³é¢‘çš„ `numpy` æ•°ç»„ã€‚<br>

The `pipeline` object expects this in **float32** format, so we convert it first to **float32**, and then extract the transcribed text.<br>

`pipeline` å¯¹è±¡æœŸæœ›ä»¥ **float32** æ ¼å¼è¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆå°†å…¶è½¬æ¢ä¸º **float32**ï¼Œç„¶åæå–è½¬å½•æ–‡æœ¬ã€‚<br>

#### 3. Create a Streaming ASR Demo with Transformers(ä½¿ç”¨Transformersåˆ›å»ºä¸€ä¸ªæµå¼ASRæ¼”ç¤º):

To make this **a streaming demo**, we need to make these changes:<br>

è¦å°†è¿™ä¸ªæ¼”ç¤ºå˜æˆæµå¼çš„ï¼Œæˆ‘ä»¬éœ€è¦åšå‡ºä»¥ä¸‹æ›´æ”¹ï¼š<br>

1. Set `streaming=True` in the Audio component(åœ¨ `Audio` ç»„ä»¶ä¸­è®¾ç½® `streaming=True`)

2. Set `live=True` in the `Interface`(åœ¨ `Interface` ä¸­è®¾ç½® `live=True`)

3. Add a **state** to the interface to store the recorded audio of a user(åœ¨interfaceä¸­æ·»åŠ ä¸€ä¸ªçŠ¶æ€stateæ¥å­˜å‚¨ç”¨æˆ·å½•åˆ¶çš„éŸ³é¢‘)

Take a look below.<br>

è¯·çœ‹ä¸‹é¢ã€‚<br>

```python
import gradio as gr
from transformers import pipeline  # ç”¨äºè°ƒç”¨é¢„è®­ç»ƒæ¨¡å‹çš„åº“
import numpy as np

# åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹
transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-base.en")

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†æµå¼éŸ³é¢‘æ•°æ®å¹¶è¿›è¡Œè¯­éŸ³è½¬å†™
def transcribe(stream, new_chunk):
    # new_chunkåŒ…å«é‡‡æ ·ç‡(sr)å’ŒéŸ³é¢‘æ•°æ®(y)
    sr, y = new_chunk
    # å°†éŸ³é¢‘æ•°æ®çš„æ•°æ®ç±»å‹è½¬æ¢ä¸ºnp.float32
    y = y.astype(np.float32)
    # å½’ä¸€åŒ–éŸ³é¢‘æ•°æ®ï¼Œä½¿å…¶æŒ¯å¹…ä½äº[-1, 1]
    y /= np.max(np.abs(y))

    # å¦‚æœstreaméç©ºï¼Œåˆ™å°†æ–°çš„éŸ³é¢‘æ•°æ®æ·»åŠ åˆ°ç°æœ‰çš„éŸ³é¢‘æ•°æ®æµä¸­
    if stream is not None:
        # å°†yæ•°ç»„åŠ åˆ°streamæ•°ç»„çš„æœ«å°¾ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„æ•°ç»„ï¼Œå¹¶å°†è¿™ä¸ªæ–°æ•°ç»„èµ‹å€¼ç»™streamå˜é‡ã€‚è¿™æ ·ï¼Œstreamå°±åŒ…å«äº†åˆ°ç›®å‰ä¸ºæ­¢æ”¶é›†åˆ°çš„æ‰€æœ‰éŸ³é¢‘æ•°æ®ã€‚
        stream = np.concatenate([stream, y])
    else:
        # å¦‚æœstreamä¸ºç©ºï¼Œåˆ™å°†æ–°çš„éŸ³é¢‘æ•°æ®åˆå§‹åŒ–ä¸ºéŸ³é¢‘æ•°æ®æµ
        stream = y
    # è¿”å›æ›´æ–°åçš„éŸ³é¢‘æ•°æ®æµï¼Œä»¥åŠä½¿ç”¨è¯­éŸ³è¯†åˆ«æ¨¡å‹è½¬å†™çš„æ–‡æœ¬
    return stream, transcriber({"sampling_rate": sr, "raw": stream})["text"]

# åˆ›å»ºä¸€ä¸ªGradioæ¥å£
demo = gr.Interface(
    # æŒ‡å®šå¤„ç†å‡½æ•°ä¸ºtranscribe
    transcribe,
    # è¾“å…¥ä¸ºä¸€ä¸ªåŒ…å«çŠ¶æ€å’ŒéŸ³é¢‘çš„åˆ—è¡¨ï¼ŒéŸ³é¢‘è¾“å…¥é€šè¿‡éº¦å…‹é£è·å–ï¼Œä»¥æµå¼å½¢å¼ä¼ è¾“
    ["state", gr.Audio(sources=["microphone"], streaming=True)],
    # è¾“å‡ºä¸ºä¸€ä¸ªåŒ…å«çŠ¶æ€å’Œæ–‡æœ¬çš„åˆ—è¡¨
    ["state", "text"],
    # è®¾ç½®ä¸ºå®æ—¶æ¨¡å¼
    live=True,
)

# å¯åŠ¨åº”ç”¨
demo.launch()
```

ğŸŸ¡ğŸŸ¡ğŸŸ¡**æ³¨æ„:** ä¸Šè¿°ä»£ç è¿è¡Œåˆ°æœ€åï¼Œå¤„ç†çš„æ˜¯å®Œæ•´çš„éŸ³é¢‘ï¼Œæ‰€ä»¥é€Ÿåº¦ä¼šå¾ˆæ…¢ã€‚å¯¹åº”çš„ï¼Œå¯ä»¥é‡‡ç”¨æ¯æ¬¡åªå¤„ç†çŸ­æš‚ç‰‡æ®µçš„æ–¹å¼ã€‚å¦‚æœæ¯æ¬¡åªå¤„ç†çŸ­æš‚ç‰‡æ®µçš„æ–¹å¼ï¼Œè¦è€ƒè™‘åˆ°ä¸Šä¸‹æ–‡ï¼Œåˆå¯èƒ½ä¼šå‡ºç°é‡å é—®é¢˜ã€‚<br>

Notice now we have a state variable now, because we need to track(è·Ÿè¸ª) all the audio history.<br>

ç°åœ¨è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªçŠ¶æ€å˜é‡ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦è·Ÿè¸ªæ‰€æœ‰çš„éŸ³é¢‘å†å²ã€‚<br>

ğŸ”¥ğŸ”¥ğŸ”¥`transcribe` gets **called** whenever there is a new small chunk of audio, but we also need to keep track of all the audio that has been spoken so far in state.<br>

ğŸ”¥ğŸ”¥ğŸ”¥æ¯å½“æœ‰ä¸€ä¸ªæ–°çš„å°éŸ³é¢‘å—æ—¶ï¼Œ`transcribe` å‡½æ•°å°±ä¼š**è¢«è°ƒç”¨**ï¼Œä½†æˆ‘ä»¬ä¹Ÿéœ€è¦åœ¨çŠ¶æ€ä¸­è·Ÿè¸ªåˆ°ç›®å‰ä¸ºæ­¢å·²ç»è¯´è¿‡çš„æ‰€æœ‰éŸ³é¢‘ã€‚<br>

As the interface runs, the `transcribe` function gets called, with a record of all the previously spoken audio in stream, as well as the new chunk of audio as `new_chunk`.<br>

éšç€ interface çš„è¿è¡Œï¼Œ`transcribe` å‡½æ•°è¢«è°ƒç”¨ï¼Œä¼´éšç€éŸ³é¢‘æµä¸­ä¹‹å‰è¯´è¿‡çš„æ‰€æœ‰éŸ³é¢‘çš„è®°å½•ï¼Œä»¥åŠä½œä¸º `new_chunk` çš„æ–°éŸ³é¢‘å—ã€‚<br>

â“â“â“We return **the new full audio** so that can be stored back in state, and we also return the transcription.<br>

â“â“â“æˆ‘ä»¬è¿”å›æ–°çš„å®Œæ•´éŸ³é¢‘ï¼Œä»¥ä¾¿å¯ä»¥å°†å…¶å­˜å‚¨å›çŠ¶æ€ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿè¿”å›è½¬å½•æ–‡æœ¬ã€‚<br>

> ä¸ºä»€ä¹ˆè¿”å›å®Œæ•´éŸ³é¢‘â“

Here we naively(ç¼ºä¹ç»éªŒã€å¤©çœŸæˆ–è¿‡äºç®€å•åŒ–çš„) append the audio together and simply call the transcriber object on the entire audio.<br>

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¤©çœŸåœ°(ä¹Ÿå°±æ˜¯ç²—æš´çš„)å°†éŸ³é¢‘è¿æ¥åœ¨ä¸€èµ·ï¼Œå¹¶ç®€å•åœ°å¯¹æ•´ä¸ªéŸ³é¢‘è°ƒç”¨ `transcriber` å¯¹è±¡ã€‚<br>

âš ï¸âš ï¸âš ï¸You can imagine more efficient(æœ‰æ•ˆçš„) ways of handling this, such as re-processing only the last 5 seconds of audio whenever a new chunk of audio received.<br>

âš ï¸âš ï¸âš ï¸ä½ å¯ä»¥æƒ³è±¡æ›´æœ‰æ•ˆçš„å¤„ç†æ–¹å¼ï¼Œä¾‹å¦‚æ¯å½“æ¥æ”¶åˆ°æ–°çš„éŸ³é¢‘å—æ—¶ï¼Œåªé‡æ–°å¤„ç†æœ€å5ç§’çš„éŸ³é¢‘ã€‚<br>

Now the ASR model will run inference as you speak!<br>

ç°åœ¨ï¼ŒASRæ¨¡å‹å°†åœ¨ä½ è¯´è¯æ—¶è¿›è¡Œæ¨ç†ï¼<br>

#### ä¸ªäººæ”¹ç‰ˆ--æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªç‰‡æ®µ:

ç”±äºGradioå®˜æ–¹Audioå®æ—¶è¯­éŸ³è¯†åˆ«ä»£ç è¿è¡Œåˆ°æœ€åï¼Œå¤„ç†çš„æ˜¯å®Œæ•´çš„éŸ³é¢‘ï¼Œé€Ÿåº¦éå¸¸éå¸¸æ…¢ï¼Œä¸ªäººè¿›è¡Œäº†æ”¹ç‰ˆï¼Œæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªå°ç‰‡æ®µã€‚ä¸ªäººæµ‹è¯•æ•ˆæœè¿˜OKã€‚<br>

ä¸‹åˆ—ä»£ç ä¸­çš„`transcribe`å‡½æ•°æ˜¯å¦èƒ½æ”¹ä¸ºå¼‚æ­¥ï¼Ÿ

```python
import gradio as gr
from transformers import pipeline
import numpy as np

print("å¼€å§‹åŠ è½½è¯­éŸ³æ¨¡å‹")
# æŒ‡å®šæ¨¡å‹çš„æœ¬åœ°è·¯å¾„
model_path = "./large-v3"   # è¿™é‡Œä½¿ç”¨çš„æ˜¯HFå¼€æºçš„"openai/whisper-large-v3"
                            # ç¬”è€…ä½¿ç”¨çš„ NVIDIA A100-PCIE-40GB "openai/whisper-large-v3" è¿è¡Œæ—¶å ç”¨çš„æ˜¾å­˜ä½ 7269MiB / 40960MiBã€‚
transcriber = pipeline("automatic-speech-recognition", model=model_path, device="cuda")
print("è¯­éŸ³æ¨¡å‹åŠ è½½æˆåŠŸğŸ…ğŸ…ğŸ…")

# åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥ä¿å­˜æ¯ä¸ªç‰‡æ®µçš„è½¬å½•æ–‡æœ¬
transcribed_texts = []

def transcribe(new_chunk):
    sr, y = new_chunk
    y = y.astype(np.float32)
    y /= np.max(np.abs(y))
    
    # ç›´æ¥ä½¿ç”¨å½“å‰çš„éŸ³é¢‘ç‰‡æ®µè¿›è¡Œè½¬å½•ï¼Œè€Œä¸æ˜¯ç´¯ç§¯éŸ³é¢‘æ•°æ®
    transcribed_text = transcriber({"sampling_rate": sr, "raw": y})["text"]
    
    # å°†è½¬å½•å‡ºçš„æ–‡æœ¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    transcribed_texts.append(transcribed_text)
    
    # è¿”å›æˆªæ­¢ç›®å‰ä¸ºæ­¢çš„è½¬å½•ç»“æœ
    return transcribed_texts

demo = gr.Interface(
    transcribe,
    gr.Audio(sources=["microphone"], streaming=True),
    "text",
    live=True,
)
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=11147)
```