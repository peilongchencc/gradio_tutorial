from transformers import WhisperForConditionalGeneration, WhisperProcessor
import torch

class AudioTranscriber:
    def __init__(self, model_path):
        """
        åˆå§‹åŒ–éŸ³é¢‘è½¬å½•å™¨ã€‚
        
        å‚æ•°:
            model_path: æ¨¡å‹å’Œå¤„ç†å™¨çš„æœ¬åœ°è·¯å¾„ã€‚
        """
        print("å¼€å§‹åŠ è½½è¯­éŸ³æ¨¡å‹")
        self.model = WhisperForConditionalGeneration.from_pretrained(model_path)
        self.processor = WhisperProcessor.from_pretrained(model_path)

        # ç¡®ä¿ä½¿ç”¨CUDAï¼Œå¦‚æœå¯ç”¨
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        print("è¯­éŸ³æ¨¡å‹åŠ è½½æˆåŠŸğŸ…ğŸ…ğŸ…")

    def transcribe(self, audio_file_path):
        """
        è½¬å½•æŒ‡å®šéŸ³é¢‘æ–‡ä»¶çš„å†…å®¹ã€‚
        
        å‚æ•°:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
            
        è¿”å›:
            éŸ³é¢‘å†…å®¹çš„æ–‡æœ¬è½¬å½•ã€‚
        """
        # é¢„å¤„ç†éŸ³é¢‘
        input_values = self.processor(audio_file_path, return_tensors="pt").input_values.to(self.device)

        # æ‰§è¡Œæ¨æ–­
        with torch.no_grad():
            logits = self.model(input_values).logits

        # åå¤„ç†ä»¥è·å–è¯†åˆ«ç»“æœ
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = self.processor.decode(predicted_ids[0])

        return transcription

# ä½¿ç”¨ç¤ºä¾‹
model_path = "./large-v3"  # æ¨¡å‹çš„æœ¬åœ°è·¯å¾„
# è¿™é‡Œä½¿ç”¨çš„æ˜¯HFå¼€æºçš„"openai/whisper-large-v3"
# ç¬”è€…ä½¿ç”¨çš„ NVIDIA A100-PCIE-40GB "openai/whisper-large-v3" è¿è¡Œæ—¶å ç”¨çš„æ˜¾å­˜ä½ 7269MiB / 40960MiBã€‚
transcriber = AudioTranscriber(model_path)

# å‡è®¾ä½ æœ‰ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„
audio_file_path = "path_to_your_audio.wav"
print("è¯†åˆ«ç»“æœ:", transcriber.transcribe(audio_file_path))
